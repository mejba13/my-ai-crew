**BRAND:** mejba.me
**TITLE:** Claude Code Simplify: My Code Got 40% Shorter
**SLUG:** claude-code-simplify-feature
**TAGS:** Claude Code, AI Development, Code Refactoring, Developer Tools, Tutorial

---

I watched my terminal do something I've never seen an AI tool do before. It tore apart code it had *just written* ‚Äî and made it better.

Not "better" in the vague, hand-wavy way people throw that word around. I'm talking about extracting reusable traits from duplicated Livewire components, converting one-at-a-time database inserts into batch operations, and collapsing bloated Blade templates into clean, modular partials. The kind of refactoring a senior developer does during a code review ‚Äî except this happened in eight and a half minutes, automatically, across an entire Laravel project.

The feature is called `/simplify`, and it shipped quietly in a recent Claude Code update. I've been running it on every project for the past week, and honestly? It's changed how I think about AI-generated code entirely. Not because the initial output was bad. But because the *second pass* catches things I would have caught myself ‚Äî three days later, in a code review, when fixing it costs ten times more effort.

Here's what most people get wrong about this feature, and there's a specific reason the "just generate better code the first time" crowd is missing the point. I'll get to that. But first, you need to understand what `/simplify` actually does under the hood ‚Äî because it's way more sophisticated than a linter.

## Why Your AI-Generated Code Needs a Second Look

I've been building with Claude Code for over a year now. My content agents, my automation workflows, client projects ‚Äî all of it runs through Claude Code at some point. And I've noticed a pattern that's hard to argue with.

First-draft code from any AI model ‚Äî Claude, GPT, Gemini, doesn't matter ‚Äî tends to solve the problem correctly but repetitively. You ask for a CRUD system, you get a create component and an edit component that share 80% of their logic but duplicate every line. You ask for a dashboard, you get the same status string hardcoded in four different files instead of pulled from a constant.

This isn't a bug. It's actually the correct behavior for a model optimizing for "give the user working code right now." The model doesn't have the full project context the way a human developer who's been living in the codebase for months does. It generates each file to be self-contained and functional.

The problem shows up at scale. When you're generating 10, 15, 20 files in a session ‚Äî like I was doing building a task management system with Laravel Livewire ‚Äî those small duplications compound into a maintenance nightmare.

I used to fix this manually. Open each file, spot the patterns, extract the shared logic, test that nothing broke. It's the kind of work that takes an afternoon and feels like it shouldn't be this tedious. That's exactly the gap `/simplify` fills ‚Äî and it does it with a multi-agent architecture that genuinely surprised me when I first saw it in action.

## Inside the Machine: How Simplify Actually Works

Here's where it gets technical, and honestly, this is the part that made me sit up and pay attention.

When you run `/simplify` in Claude Code, it doesn't just scan your files with a regex or run a basic linter. The system kicks off a multi-step analysis pipeline that works like this:

**Step 1: Git Diff Capture.** Simplify looks at your latest changes ‚Äî everything in your working tree that's been modified since your last commit. This scoping is smart because it means the tool only analyzes code you've recently touched, not your entire 500-file project.

**Step 2: Three Parallel Review Agents.** This is the part that blew my mind. The system launches three independent agents, each running on Sonnet for speed, and each focused on a different lens:

- **Agent 1 ‚Äî Code Reuse:** Scans for duplicated logic across files. Looks for functions, methods, or template blocks that appear in multiple places with minor variations. Proposes extractions into shared traits, components, or utilities.

- **Agent 2 ‚Äî Code Quality:** Evaluates naming conventions, structural patterns, proper use of framework features. Catches things like using raw strings instead of constants, or building something from scratch when the framework already has a built-in.

- **Agent 3 ‚Äî Code Efficiency:** Focuses on performance. Database queries inside loops, unnecessary re-renders, operations that could be batched, caching opportunities.

**Step 3: Main Agent Synthesis.** The findings from all three agents get fed into a primary reasoning model that synthesizes their recommendations, resolves any conflicts between suggestions, and produces a prioritized list of fixes.

**Step 4: Implementation.** Claude Code applies the changes ‚Äî with your approval ‚Äî directly to your files. No copy-pasting. No switching between tools. Just a git diff you can review.

The whole process took 8 minutes and 36 seconds on my Laravel project. That's not instant, but consider what happened in those eight minutes: three separate AI analysts reviewed every file I'd changed, cross-referenced patterns across the entire diff, and produced actionable refactoring that would have taken me ‚Äî being honest here ‚Äî at least two hours of focused review work.

That trade-off makes sense to me. Every time.

## My Laravel Project: Eight Fixes That Actually Mattered

Let me walk you through exactly what `/simplify` found on my task management project. I was building a Livewire-based CRUD system ‚Äî create tasks, edit tasks, dashboard to view them ‚Äî and had just finished the initial generation pass. Everything worked. Tests passed. The code was fine.

"Fine" is the enemy of good, and `/simplify` showed me exactly where.

**Fix 1: Constants Instead of Magic Strings**

The Task model had status and priority values scattered across files as raw strings ‚Äî "pending", "in_progress", "completed" in the controller, the same strings in the Blade templates, again in the dashboard filter logic. Simplify proposed adding constants to the model itself.

```php
class Task extends Model
{
    const STATUS_PENDING = 'pending';
    const STATUS_IN_PROGRESS = 'in_progress';
    const STATUS_COMPLETED = 'completed';

    const PRIORITY_LOW = 'low';
    const PRIORITY_MEDIUM = 'medium';
    const PRIORITY_HIGH = 'high';
}
```

Now, I'd actually prefer PHP enums here ‚Äî which launched in PHP 8.1 and are the modern approach ‚Äî but the principle is dead right. Centralizing these values means you change them in one place, and every reference updates. The fact that an AI caught this across multiple files simultaneously is impressive.

**Pro tip:** If you're on PHP 8.1+, take Simplify's constant suggestion and upgrade it to a backed enum. You get type safety for free.

**Fix 2: Extracting a Shared Form Trait**

This was the big one. My `CreateTask` and `EditTask` Livewire components shared about 80% of their form handling logic ‚Äî validation rules, property definitions, the save workflow. Simplify proposed a `HasTaskForm` trait that both components could use.

```php
trait HasTaskForm
{
    public string $title = '';
    public string $description = '';
    public string $status = 'pending';
    public string $priority = 'medium';

    protected function taskRules(): array
    {
        return [
            'title' => 'required|min:3|max:255',
            'description' => 'required|min:10',
            'status' => 'required|in:pending,in_progress,completed',
            'priority' => 'required|in:low,medium,high',
        ];
    }

    protected function fillFromTask(Task $task): void
    {
        $this->title = $task->title;
        $this->description = $task->description;
        $this->status = $task->status;
        $this->priority = $task->priority;
    }
}
```

Before this extraction, changing a validation rule meant updating two files. Missing one would create subtle, infuriating bugs where creating a task enforced different rules than editing one. I've seen this exact issue in production apps. It's always embarrassing.

**Fix 3: Dashboard Consistency with Model Constants**

Once the constants existed in the model, Simplify updated the dashboard component to reference them instead of raw strings. Small change, massive impact on maintainability:

```php
// Before
$tasks->where('status', 'completed')->count();

// After
$tasks->where('status', Task::STATUS_COMPLETED)->count();
```

This is the kind of fix that takes 30 seconds to make but prevents hours of debugging when someone decides "completed" should actually be "done" six months from now.

**Fix 4: Proper Component Usage in Blade**

I had a button wrapped in an anchor tag ‚Äî `<a href="..."><button>Create Task</button></a>`. Classic HTML anti-pattern. Simplify noticed I was using the Flux UI component library and proposed converting it to the proper Flux button component with built-in navigation.

Small? Yes. But it's exactly the kind of thing that causes accessibility issues and fails HTML validation. A senior developer would catch it in code review. Now the AI catches it first.

**Fix 5: Extracting a Reusable Task Row Component**

My dashboard Blade template had the same div structure repeated for each task display ‚Äî status badge, title, priority indicator, action buttons. Same HTML, copy-pasted with minor variable changes. Simplify proposed an `<x-task-row>` Blade component:

```blade
<!-- Before: 40 lines repeated 3 times -->
<!-- After: -->
<x-task-row :task="$task" />
```

The component encapsulates all the display logic. Change how tasks look? Edit one file. Add a new action button? One place. This alone cut about 120 lines from my dashboard template.

**Fix 6: Shared Form Partial for Create and Edit Views**

Similar to the trait extraction, the Blade templates for create and edit forms were nearly identical. Simplify proposed a single `_task-form.blade.php` partial that accepts an optional `$task` parameter:

```blade
@include('tasks._task-form', ['task' => $task ?? null])
```

The partial conditionally pre-fills fields when editing and leaves them empty when creating. One template, two use cases. Clean.

**Fix 7: Cleaning Up Verbose Include Statements**

Some of my Blade `@include` directives had gotten unnecessarily complex ‚Äî long paths, nested data arrays that could be simplified. Simplify shortened them without changing behavior. A readability win.

**Fix 8: Batch Database Operations**

This one was the performance fix. My seeder was creating tasks one at a time inside a loop:

```php
// Before
foreach ($taskData as $data) {
    Task::create($data);
}

// After
Task::insert($taskData);
```

One query instead of N queries. On a seeder with 50 sample tasks, that's the difference between 50 database round-trips and one. Scale that to production data operations and you're looking at meaningful performance gains.

If you've followed along through all eight fixes, you'll notice something: none of these are bugs. Every single change is about making working code *better*. That's a fundamentally different kind of AI assistance than we've been trained to expect, and it's why I think this feature matters more than most people realize.

## Running Simplify on Your Own Projects: Step by Step

Want to try this yourself? Here's exactly how I run it, including the gotchas I've discovered.

**1. Make sure you have recent changes in your git working tree.**

Simplify operates on your diff ‚Äî the difference between your current files and your last commit. If you've already committed everything, there's nothing to analyze. I typically run Simplify *before* committing, treating it as a pre-commit review step.

```bash
# Check that you have uncommitted changes
git status
git diff --stat
```

**2. Run the simplify command in Claude Code.**

```
/simplify
```

That's it. One command. Claude Code handles the rest ‚Äî capturing your diff, launching the review agents, synthesizing the results.

**3. Wait for the analysis.**

This takes anywhere from 3-10 minutes depending on how much code changed. My experience with a medium-sized Laravel project (about 15 modified files) was roughly 8.5 minutes. Larger diffs will take longer.

Don't panic if it seems slow. Three agents are running in parallel, each doing a thorough analysis of your code. Speed isn't the goal here ‚Äî depth is.

**4. Review the suggestions.**

Simplify presents its findings as a numbered list of proposed changes, each with:
- What it wants to change
- Why the change improves the code
- The specific files affected

**5. Approve or modify.**

You can accept all suggestions, cherry-pick specific ones, or ask Claude Code to modify a suggestion before applying it. I almost always accept the structural changes (trait extraction, component creation) but sometimes tweak the naming.

**Pro tip:** Run `git diff` after Simplify applies its changes. Read through the diff carefully. This is where you learn the most ‚Äî seeing what patterns the AI caught that you missed trains your own code review eye over time.

**Common gotcha:** If you're working on a large feature branch with hundreds of changes, consider running Simplify in stages. Do your model layer work, run Simplify, commit. Do your component layer, run Simplify, commit. The analysis is more focused and the suggestions are more actionable when the diff is scoped.

## The Cost Question Nobody's Ignoring

Let's talk about tokens, because this matters.

Running `/simplify` on my Laravel project consumed approximately 8% of my session's token budget. I'm on the $100/month Claude plan with the 5x Anthropic multiplier, which gives generous headroom ‚Äî but 8% for a single operation is not nothing.

Here's how I think about the economics. Two hours of my time doing manual code review at my consulting rate far exceeds the fractional cost of tokens. Even if you're not billing hourly, consider the opportunity cost. Those two hours spent extracting traits and creating Blade components are two hours you're not shipping features or taking on new work.

The token usage also scales with your diff size. A focused set of changes to 5-6 files might consume 3-4%. A massive 30-file generation pass might hit 12-15%. Planning your work in smaller batches keeps the cost per Simplify run lower and the results more targeted.

One thing I'd like to see in future updates: a token estimate *before* the analysis runs, so you can make an informed choice on larger diffs. Right now, you commit to the cost when you hit enter.

## The "Just Write Better Code" Debate

I need to address this because it comes up every time someone demos Simplify.

The criticism goes something like: "Why does the AI need a second pass to fix its own code? Just make the model generate better code the first time." I've seen variations of this take from developers I respect. And I get the instinct ‚Äî it feels like a hack.

But here's the thing. This argument misunderstands how code generation works at a fundamental level.

When you ask an AI to generate a CRUD system, it generates each component to be correct and self-contained. The create form works. The edit form works. The dashboard works. Each piece is individually sound. The *duplication* only becomes visible when you look across the entire system ‚Äî when you hold up the create component next to the edit component and notice they share 80% of their code.

This is exactly what human developers do. We write the first implementation to make it work. Then we refactor to make it clean. "Make it work, make it right, make it fast" is literally a programming maxim that predates AI by decades.

The simplify feature is the "make it right" step, automated.

I'd actually argue that trying to generate perfectly refactored code on the first pass would produce *worse* results. The model would have to simultaneously solve the functional problem AND optimize for cross-file patterns, increasing the chance of subtle bugs. Separating generation from optimization is good engineering ‚Äî for humans and AIs alike.

What changed my mind on this was watching the three review agents work. Each one has a narrow focus ‚Äî reuse, quality, efficiency ‚Äî and they catch different things. A single-pass model trying to optimize for all three simultaneously would make trade-offs that a multi-agent approach avoids. It's the same reason code reviews work better than self-reviews. Fresh eyes, different priorities.

That said, I do think the criticism points to a real tension in AI-assisted development. We want these tools to be magical, to produce perfect output in one shot. The reality is messier and more iterative. Accepting that doesn't mean settling ‚Äî it means building workflows that account for it.

## What I've Measured After a Week of Simplify

I ran `/simplify` on five different projects over the past week. Three Laravel apps, one Next.js project, and one Python automation script. Here's what I observed.

**Code reduction:** Across all five projects, Simplify reduced total lines of code by 15-40%. The largest reduction was the Laravel Livewire project (the one I detailed above) at roughly 38%. The smallest was the Python script at about 15% ‚Äî there was less duplication to extract.

**Unique components created:** Simplify proposed creating 3-7 new shared components, traits, or utilities per project. Not all of these were worth keeping ‚Äî sometimes the abstraction was premature for a codebase that small ‚Äî but most were genuinely useful.

**Bugs prevented:** Hard to quantify, but I found two cases where the constant extraction caught inconsistent string values between files. These would have become bugs eventually. Probably during a demo.

**Time investment:** 3-10 minutes per run, depending on diff size. Total active time per project (reviewing and adjusting suggestions) was about 15-20 minutes. Compare to the 1-2 hours of manual refactoring this replaced.

**Token cost:** 3-12% of session budget per run. Averaging about 7% across my usage.

The numbers tell a clear story, but the real value is harder to measure. The codebase *feels* different after Simplify runs on it. Files are shorter. Patterns are consistent. When you need to make a change, there's one place to make it, not four. That compounds over every future editing session.

## Where This Falls Short (And What I'd Change)

I'd be doing you a disservice if I didn't mention the limitations, because there are real ones.

**Simplify can over-abstract.** On my smaller Python project, it suggested extracting a helper function that was only used in two places. The function had three parameters, and calling it was barely shorter than the inline code it replaced. I rejected that suggestion. Not every pattern deserves an abstraction ‚Äî sometimes two similar blocks of code are similar by coincidence, not design.

**The token cost adds up during heavy generation sessions.** If you're in a marathon coding session generating an entire application, running Simplify multiple times can eat 30-40% of your token budget. I've started being more strategic about *when* I run it ‚Äî typically at natural breakpoints, not after every small change.

**No preview of token cost.** As I mentioned, you can't see how many tokens Simplify will consume before it runs. I'd love a `--estimate` flag that shows the projected cost based on diff size.

**Framework-specific suggestions vary in quality.** The Laravel suggestions were excellent ‚Äî the tool clearly understands the framework's patterns and conventions. The Next.js suggestions were good but occasionally missed framework-specific optimizations like Server Components. Your mileage will vary depending on your stack.

**It can't catch architectural problems.** Simplify works at the code level ‚Äî extracting, consolidating, optimizing. It doesn't tell you that your entire approach to state management is wrong, or that you should be using a different pattern altogether. That's still a human judgment call.

These are real limitations, but none of them are dealbreakers. They're the kind of rough edges I expect to smooth out over the next few updates.

## This Changes How I Work With AI Code Generation

Here's what surprised me most about adopting `/simplify` ‚Äî it didn't just improve my code. It improved my *workflow*.

Before Simplify, my process was: generate code, manually review every file, refactor the obvious duplication, commit. The manual review step was the bottleneck, and I'll be honest ‚Äî on tired afternoons, I'd sometimes skip it. Ship the working-but-messy code and promise myself I'd clean it up later. (I wouldn't.)

Now my process is: generate code, run `/simplify`, review the *suggestions* instead of the raw code, approve, commit. The review step is faster because I'm evaluating proposed changes, not hunting for problems. And I never skip it because running a one-word command has zero friction.

The deeper insight is this: AI-assisted development isn't a single-step process, and the sooner we accept that, the better our code gets. Generation is step one. Review and refinement ‚Äî whether by a human, an AI, or both ‚Äî is step two. Simplify automates step two in a way that's genuinely useful, not just cosmetic.

If you're building anything with Claude Code ‚Äî even small projects ‚Äî try running `/simplify` on your next generation pass. Look at the diff. See what it catches. I think you'll be surprised, the same way I was, at the patterns hiding in plain sight.

And the next time someone tells you AI-generated code is inherently messy, you'll have a one-word response.

---

## ü§ù Let's Work Together

Looking to build AI systems, automate workflows, or scale your tech infrastructure? I'd love to help.

* üîó **Fiverr** (custom builds & integrations): [fiverr.com/s/EgxYmWD](https://www.fiverr.com/s/EgxYmWD)
* üåê **Portfolio**: [mejba.me](https://www.mejba.me)
* üè¢ **Ramlit Limited** (enterprise solutions): [ramlit.com](https://www.ramlit.com)
* üé® **ColorPark** (design & branding): [colorpark.io](https://www.colorpark.io)
* üõ° **xCyberSecurity** (security services): [xcybersecurity.io](https://www.xcybersecurity.io)
